\input{Config/preamble}
\input{Config/format}
\input{Config/commands}

\usepackage{booktabs}
\usepackage{diagbox}

% 
\begin{document}

{\scshape\bf\textcolor{darkred}{PROVA II - Probabilidade (PPGECD000000001)}}


\textsf{\textbf{Professor:}} \text{Raydonal Ospina Martinez.}  \quad {\bf\small E-mail:} \href{mailto:raydonal@castlab.org}{\texttt{raydonal@castlab.org}}

\bigskip 
\begin{regras}
Recomendo que leiam atentamente as perguntas e reservem um tempo adequado para refletir sobre elas. Ressalto que todas as quest√µes devem ser respondidas de forma detalhada, pois solu√ß√µes amb√≠guas ou pouco claras ser√£o penalizadas. Lembro ainda que n√£o farei esfor√ßo para interpretar ou ``adivinhar'' o que o aluno quis escrever ou dizer. Por isso, √© fundamental que sejam claros e organizados. Informo que a prova dever√° ser entregue (digitalizada em formato PDF) no dia {\bf 26/06/2025} at√© as 21:00h (GMT-3 Hor√°rio de Bras√≠lia). Dever√£o encaminhar para o e-mail acima com assunto de envio ({\tt Resposta Prova II - Mestrado - ``coloque aqui seu  nome''})  e colocando seu nome.
\end{regras}
\vspace{2ex}


\begin{problema}{}{pr-1}
	Seja $\Omega = \{a, b, c\}$  um espa√ßo amostral, ${\cal F}={\cal P}(\Omega)$ o conjunto de partes de $\Omega$ como sua $\sigma$-√°lgebra e $P(\{\omega\})=\frac{1}{3}$ para todo $\omega \in \Omega.$ Consideremos as vari√°veis aleat√≥rias $X$ e $Y$ definidas em $(\Omega, {\cal F}, P)$ como
	$$
	X(\omega) =
	\begin{cases}
		1, &  \text{se}  \quad  \omega=a, \ \text{ou} \ \omega = b,  \\
		0, &  \text{se}  \quad  \omega=c
	\end{cases} 
	\qquad \text{e} \qquad 
	Y(\omega) =
	\begin{cases}
		\ \pi, &  \text{se}  \quad  \omega=a, \\
		\ \frac{1}{2}, &  \text{se}  \quad  \omega=b, \\
		-1, &  \text{se}  \quad  \omega=c
	\end{cases} 
	$$
	Obtenha as distribui√ß√µes condicionais acumuladas $F(X|Y)$ e $F(Y|X)$  
	
	{\bf Dica:}  Note que as vari√°veis $X$ e $Y$ s√£o discretas.
	
\end{problema}


{\small {\bf Res.}
A fun√ß√£o de probabilidade conjunta, $p(x, y) = P(X=x, Y=y)$, √© determinada avaliando os pares $(X(\omega), Y(\omega))$ para cada $\omega \in \Omega.$ Dado que $P(\{a\}) = P(\{b\}) = P(\{c\}) = 1/3$ temos: \begin{itemize}
	\item Para $\omega = a$: Temos $X(a) = 1$ e $Y(a) = \pi$. Ent√£o  $P(\{a\}) = 1/3$. Logo, $p(1, \pi) = P(X=1, Y=\pi) = 1/3$.
	
	\item Para $\omega = b$: Temos $X(b) = 1$ e $Y(b) = 1/2$. Ent√£o √© $P(\{b\}) = 1/3$. Logo, $p(1, 1/2) = P(X=1, Y=1/2) = 1/3$.
	
	\item Para $\omega = c$: Temos $X(c) = 0$ e $Y(c) = -1$. Ent√£o $P(\{c\}) = 1/3$. Logo, $p(0, -1) = P(X=0, Y=-1) = 1/3$.
\end{itemize} Como as vari√°veis s√£o discretas a distribui√ß√£o conjunta pode ser representada na tabela \begin{center}
	\begin{tabular}{c|ccc|c}
		\toprule
		\diagbox[height=0.7cm]{$X$}{$Y$} & -1 & 1/2 & $\pi$ & $p_X(x)$ \\
		\midrule
		0 & 1/3 & 0 & 0 & 1/3 \\
		1 & 0 & 1/3 & 1/3 & 2/3 \\
		\midrule
		$p_Y(y)$ & 1/3 & 1/3 & 1/3 & 1 \\
		\bottomrule
	\end{tabular}
\end{center}
A fun√ß√£o de probabilidade condicional de $X$ dado $Y$ √© dada por $p(x|y) = \frac{p(x,y)}{p_Y(y)}$ e $p(y|x) = \frac{p(x,y)}{p_X(x)}$, i.e.,
\begin{itemize}
	\item Se $y=-1$: $p(x|-1) = \frac{p(x,-1)}{1/3}$. Logo, $p(0|-1)=1$ e $p(1|-1)=0$.
	\item Se $y=1/2$: $p(x|1/2) = \frac{p(x,1/2)}{1/3}$. Logo, $p(0|1/2)=0$ e $p(1|1/2)=1$.
	\item Se $y=\pi$: $p(x|\pi) = \frac{p(x,\pi)}{1/3}$. Logo, $p(0|\pi)=0$ e $p(1|\pi)=1$.
\end{itemize}Por outro lado, A fun√ß√£o de probabilidade condicional de $Y$ dado $X$ √©
\begin{itemize}
	\item Se $x=0$: $p(y|0) = \frac{p(0,y)}{1/3}$. Logo, $p(-1|0)=1$ e $p(y|0)=0$ para $y \neq -1$.
	\item Se $x=1$: $p(y|1) = \frac{p(1,y)}{2/3}$. Logo, $p(1/2|1)=1/2$ e $p(\pi|1)=1/2$.
\end{itemize}
Agora, somente precisamos acumular a partir das probabilidades condicionais, i.e. olhar para cada valor de $y$, ou seja $F(x|y) = P(X \le x | Y=y)$
\begin{itemize}
	\item Para $y=-1$: a probabilidade est√° concentrada em $X=0$.
	$$ F(x|-1) = 
	\begin{cases}
		0, & \text{se } x < 0 \\
		1, & \text{se } x \ge 0
	\end{cases}
	$$
	\item Para $y=1/2$: a probabilidade est√° concentrada em $X=1$.
	$$ F(x|1/2) = 
	\begin{cases}
		0, & \text{se } x < 1 \\
		1, & \text{se } x \ge 1
	\end{cases}
	$$
	\item Para $y=\pi$: a probabilidade est√° concentrada em $X=1$.
	$$ F(x|\pi) = 
	\begin{cases}
		0, & \text{se } x < 1 \\
		1, & \text{se } x \ge 1
	\end{cases}
	$$
\end{itemize}

De forma an√°loga $F(y|x) = P(Y \le y | X=x)$, i.e. precisamos olhar para cada $x$.
\begin{itemize}
	\item Para $x=0$: a probabilidade est√° concentrada em $Y=-1$.
	$$ F(y|0) = 
	\begin{cases}
		0, & \text{se } y < -1 \\
		1, & \text{se } y \ge -1
	\end{cases}
	$$
	\item Para $x=1$: a probabilidade est√° concentrada em $Y=1/2$ e $Y=\pi$.
	$$ F(y|1) = 
	\begin{cases}
		0, & \text{se } y < 1/2 \\
		P(Y \le y | X=1) = p(1/2|1) = 1/2, & \text{se } 1/2 \le y < \pi \\
		P(Y \le y | X=1) = p(1/2|1) + p(\pi|1) = 1, & \text{se } y \ge \pi
	\end{cases}
	$$
\end{itemize}

}



\begin{problema}{}{pr-3}
Suponha que a distribui√ß√£o conjunta das vari√°veis aleat√≥rias discretas $X$ e $Y$ est√° dada por
\begin{center}
	\begin{tabular}{|c|c|c|c|c|} \hline
		$X \diagdown Y$ & 1 & 2 & 3 & 4 \\ \hline
		0 & 0,1 & 0 & 0 & 0 \\ \hline
		-1 & 0,1 & 0,1 & 0 & 0 \\ \hline
		-2 & 0,1 & 0,1 & 0,1 & 0 \\ \hline
		-3 & 0,1 & 0,1 & 0,1 & 0,1 \\ \hline
	\end{tabular}
\end{center}
Calcule:
\begin{enumerate}
	\item $ P(X \geq -1, Y \geq 1) $
	\item As distribui√ß√µes marginais de $X$ e $Y$ e determine se $X$ e $Y$ s√£o independentes.
	\item Encontre a fun√ß√£o de distribui√ß√£o condicional de $X$ dado $Y.$
\end{enumerate}
%
\end{problema}

{\small {\bf Res.} Seja $p(x,y)$ a fun√ß√£o de probabilidade conjunta dada na tabela.

{\bf  (a):} C√°lculo de $ P(X \geq -1, Y \geq 1) $
O evento $\{X \ge -1, Y \ge 1\}$ contem os pares $(x,y)$ tais que $x \in \{0, -1\}$ e $y \in \{1, 2, 3, 4\}$. A probabilidade √© a soma das probabilidades conjuntas para esses pares.
\begin{align*}
	P(X \ge -1, Y \ge 1) &= \sum_{x \in \{0,-1\}} \sum_{y=1}^{4} p(x,y) \\
	&= p(0,1) + p(0,2) + p(0,3) + p(0,4) \\
	&\quad + p(-1,1) + p(-1,2) + p(-1,3) + p(-1,4) \\
	&= (0,1 + 0 + 0 + 0) + (0,1 + 0,1 + 0 + 0) \\
	&= 0,1 + 0,2 = 0,3
\end{align*}
Portanto, $\mathbf{P(X \geq -1, Y \geq 1) = 0,3}$.

{\bf  (b):} As distribui√ß√µes marginais, $p_X(x)$ e $p_Y(y)$, s√£o obtidas somando as probabilidades ao longo das linhas e colunas da tabela conjunta, respectivamente.
\begin{center}
	\begin{tabular}{c|cccc|c}
		\toprule
		\diagbox[height=0.7cm]{$X$}{$Y$} & 1 & 2 & 3 & 4 & $p_X(x)$ \\
		\midrule
		0 & 0,1 & 0 & 0 & 0 & 0,1 \\
		-1 & 0,1 & 0,1 & 0 & 0 & 0,2 \\
		-2 & 0,1 & 0,1 & 0,1 & 0 & 0,3 \\
		-3 & 0,1 & 0,1 & 0,1 & 0,1 & 0,4 \\
		\midrule
		$p_Y(y)$ & 0,4 & 0,3 & 0,2 & 0,1 & 1,0 \\
		\bottomrule
	\end{tabular}
\end{center}
Assim, a distribui√ß√£o marginal de $X$ √©
$p_X(0) = 0,1$; \quad $p_X(-1) = 0,2$; \quad $p_X(-2) = 0,3$; \quad $p_X(-3) = 0,4$.
Por outro lado, a distribui√ß√£o marginal de $Y$ √©
$p_Y(1) = 0,4$; \quad $p_Y(2) = 0,3$; \quad $p_Y(3) = 0,2$; \quad $p_Y(4) = 0,1$.

Agora, sabemos que duas vari√°veis aleat√≥rias $X$ e $Y$ s√£o independentes se, e somente se, $p(x,y) = p_X(x)p_Y(y)$ para \textbf{todos} os pares $(x,y)$. √â suficiente encontrar um contra-exemplo.
Considere o par $(x,y) = (0,1).$: Da tabela,  temos que $p(0,1) = 0,1,$ e 
	o produto das marginais √© $p_X(0) \cdot p_Y(1) = (0,1) \times (0,4) = 0,04$. Como $p(0,1) = 0,1 \neq 0,04 = p_X(0)p_Y(1)$, logo $X$ e $Y$ n√£o s√£o independentes.

{\bf (c):} A fun√ß√£o de distribui√ß√£o condicional de $X$ dado $Y,$ $F(x|y) = P(X \le x | Y=y)$ √© obtida da distribui√ß√£o condicional $p(x|y) = p(x,y)/p_Y(y)$.

Para $Y = 1$ ($p_Y(1)=0,4$)
$p(0|1) = \frac{0,1}{0,4} = \frac{1}{4}$; $p(-1|1) = \frac{0,1}{0,4} = \frac{1}{4}$; $p(-2|1) = \frac{0,1}{0,4} = \frac{1}{4}$; $p(-3|1) = \frac{0,1}{0,4} = \frac{1}{4}$.

$F(x|1) = \begin{cases}
	0, & x < -3 \\ 1/4, & -3 \le x < -2 \\ 1/4+1/4 = 1/2, & -2 \le x < -1 \\ 1/2+1/4 = 3/4, & -1 \le x < 0 \\ 1, & x \ge 0
\end{cases}$

para $Y = 2$ ($p_Y(2)=0,3$)
$p(-1|2)=\frac{0,1}{0,3}=\frac{1}{3}$; $p(-2|2)=\frac{0,1}{0,3}=\frac{1}{3}$; $p(-3|2)=\frac{0,1}{0,3}=\frac{1}{3}$.

$F(x|2) = \begin{cases}
	0, & x < -3 \\ 1/3, & -3 \le x < -2 \\ 1/3+1/3=2/3, & -2 \le x < -1 \\ 1, & x \ge -1
\end{cases}$

para $Y = 3$ ($p_Y(3)=0,2$)
$p(-2|3)=\frac{0,1}{0,2}=\frac{1}{2}$; $p(-3|3)=\frac{0,1}{0,2}=\frac{1}{2}$.

$F(x|3) = \begin{cases}
	0, & x < -3 \\ 1/2, & -3 \le x < -2 \\ 1, & x \ge -2
\end{cases}$

para  $Y = 4$ ($p_Y(4)=0,1$)
$p(-3|4)=\frac{0,1}{0,1}=1$.

$F(x|4) = \begin{cases}
	0, & x < -3 \\
	1, & x \ge -3
\end{cases}$

}





\begin{problema}{}{pr-2}
	Considere um par de vari√°veis aleat√≥rias discretas $(X, Y)$ cuja fun√ß√£o de distribui√ß√£o de probabilidade conjunta √© $F$, i.e., $F(x,y)=P(X\leq x, Y\leq y),$ $x,y \in \mathbb{R}.$ Sejam $F_X$ e $F_Y$ as fun√ß√µes de distribui√ß√£o das vari√°veis aleat√≥rias $X$ e $Y,$ respectivamente (distribui√ß√µes marginais). Mostre que: $$P(X>x, Y>y)= 1-F_X(x)-F_Y(y)+F(x,y).$$ 	
\end{problema}

{\small {\bf Res.} Sejam $A$ e $B$ os eventos:
\begin{itemize}
	\item $A = \{ \omega \in \Omega : X(\omega) > x \}$
	\item $B = \{ \omega \in \Omega : Y(\omega) > y \}$
\end{itemize} O nosso objetivo √© calcular $P(A \cap B)$. Note que
\begin{itemize}
	\item $A^c = \{ \omega \in \Omega : X(\omega) \le x \}$. Por defini√ß√£o, $P(A^c) = P(X \le x) = F_X(x)$.
	\item $B^c = \{ \omega \in \Omega : Y(\omega) \le y \}$. Por defini√ß√£o, $P(B^c) = P(Y \le y) = F_Y(y)$.
\end{itemize}
Mas  sabemos que $$ P(A \cap B) = 1 - P((A \cap B)^c) $$ e 
utilizando as Leis de De Morgan para conjuntos, sabemos que $(A \cap B)^c = A^c \cup B^c$. Da√≠, 
$$ P(A \cap B) = 1 - P(A^c \cup B^c) $$
Agora, aplicamos o Princ√≠pio da Inclus√£o-Exclus√£o para a probabilidade da uni√£o de dois eventos:
$$ P(A^c \cup B^c) = P(A^c) + P(B^c) - P(A^c \cap B^c) $$
Assim, $P(A^c) = F_X(x),$ $P(B^c) = F_Y(y)$ e $A^c \cap B^c = \{X \le x \text{ e } Y \le y\}$. A probabilidade deste evento √©, por defini√ß√£o da fun√ß√£o de distribui√ß√£o conjunta, $P(A^c \cap B^c) = P(X \le x, Y \le y) = F(x,y)$. Substituindo estes termos de volta na f√≥rmula da uni√£o:
$$ P(A^c \cup B^c) = F_X(x) + F_Y(y) - F(x,y) $$
como  $ P(A \cap B) = 1 - \left( F_X(x) + F_Y(y) - F(x,y) \right) $ temos
$$ P(X>x, Y>y) = 1 - F_X(x) - F_Y(y) + F(x,y) $$
}



\begin{problema}{}{pr5}
Sejam $X$ e $Y$ vari√°veis aleat√≥rias com fun√ß√£o de densidade de probabilidade conjunta dada por 
$$ f(x,y) = 
\begin{cases}
	\frac{1}{4}, & \ \text{se} \  \  -1<x<1,  -1<y<1, \\
	0, &  \text{caso contr√°rio.}
\end{cases}
$$

\begin{enumerate}
	\item  Obtenha $P(X+Y> 0)$ e $P(X>0).$
	\item Sejam $Z=X+Y$ e $W=X-Y$ fun√ß√µes lineares das var√°veis aleat√≥rias $X$ e $Y.$ Usando o m√©todo do Jacobiano obtenha a fun√ß√£o de densidade conjunta de $Z$ e $W.$
		\item Obtenha a fun√ß√£o de densidade (marginal) de $W$ 
	\item  Obtenha a fun√ß√£o de densidade condicional de $Z$ dado $W,$ i.e., $f_{Z|W}(z|w).$
\end{enumerate}
\end{problema}

{\small {\bf Res. }
	
	{\bf (a)} Como a distribui√ß√£o √© uniforme, a probabilidade de um evento $A \subseteq S$ √© dada por $P(A) = \text{√Årea}(A) / \text{√Årea}(S)$. O evento corresponde √† regi√£o $R_1 = \{ (x,y) \in S : x+y > 0 \}$, ou $y > -x$. A linha $y=-x$ divide o quadrado $S$ em duas metades de √°rea igual. A regi√£o $y > -x$ √© a metade superior do quadrado. Portanto, a √Årea($R_1$) = $\frac{1}{2} \text{√Årea}(S) = \frac{1}{2} \times 4 = 2$. Assim. a probabilidade $ P(X+Y>0) = \frac{\text{√Årea}(R_1)}{\text{√Årea}(S)} = \frac{2}{4} = \frac{1}{2}$.
	
	Agora para $P(X>0),$ o evento corresponde √† regi√£o $R_2 = \{ (x,y) \in S : x > 0 \}$. Esta regi√£o √© o ret√¢ngulo $[0,1] \times [-1,1]$, que √© a metade direita do quadrado $S$. A √°rea √© $\text{√Årea}(R_2) = (1-0) \times (1-(-1)) = 1 \times 2 = 2$.
	A probabilidade √©: $ P(X>0) = \frac{\text{√Årea}(R_2)}{\text{√Årea}(S)} = \frac{2}{4} = \frac{1}{2}$.
	
	{\bf (b)}
	Dada a transforma√ß√£o $Z=X+Y$ e $W=X-Y$, resolvemos para $X$ e $Y$:
	\begin{itemize}
		\item $Z+W = (X+Y)+(X-Y) = 2X \implies X = \frac{Z+W}{2}$
		\item $Z-W = (X+Y)-(X-Y) = 2Y \implies Y = \frac{Z-W}{2}$
	\end{itemize}
	O Jacobiano $J$ √© o determinante da matriz de derivadas parciais de $(x,y)$ em rela√ß√£o a $(z,w)$:
	$$ J = \det \begin{pmatrix} \frac{\partial x}{\partial z} & \frac{\partial x}{\partial w} \\ \frac{\partial y}{\partial z} & \frac{\partial y}{\partial w} \end{pmatrix} = \det \begin{pmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \end{pmatrix} = \left(\frac{1}{2}\right)\left(-\frac{1}{2}\right) - \left(\frac{1}{2}\right)\left(\frac{1}{2}\right) = -\frac{1}{4} - \frac{1}{4} = -\frac{1}{2} $$
	O valor absoluto do Jacobiano √© $|J| = 1/2$. O suporte original √© $-1<x<1$ e $-1<y<1$. Substitu√≠mos $X$ e $Y$:
	\begin{itemize}
		\item $-1 < \frac{z+w}{2} < 1 \implies -2 < z+w < 2$
		\item $-1 < \frac{z-w}{2} < 1 \implies -2 < z-w < 2$
	\end{itemize}
	Esta regi√£o $S'$ no plano $(z,w)$ √© um quadrado rotacionado com v√©rtices em $(2,0), (0,2), (-2,0), (0,-2)$.
	
	Agora, a densidade √© $f_{Z,W}(z,w) = f_{X,Y}(x(z,w), y(z,w)) \cdot |J|$.
	$$ f_{Z,W}(z,w) = \frac{1}{4} \cdot \frac{1}{2} = \frac{1}{8} $$
	Assim, a fun√ß√£o de densidade conjunta de $(Z,W)$ √©:
	$$ f_{Z,W}(z,w) = 
	\begin{cases}
		\frac{1}{8}, & \text{se } (z,w) \in S' \\
		0, &  \text{caso contr√°rio.}
	\end{cases}
	$$
	
 {\bf (c)} 
	Para obter $f_W(w)$, integramos $f_{Z,W}(z,w)$ sobre $z$. Para um $w \in (-2,2)$ fixo, os limites de $z$ s√£o dados por $-2 < z+w < 2 \implies -2-w < z < 2-w$ e $-2 < z-w < 2 \implies w-2 < z < w+2$. Combinando: $\max(-2-w, w-2) < z < \min(2-w, w+2),$ i..e, $-2+|w| < z < 2-|w|$.
	\begin{align*}
		f_W(w) = \int_{-\infty}^{\infty} f_{Z,W}(z,w) \,dz &= \int_{-2+|w|}^{2-|w|} \frac{1}{8} \,dz \\
		&= \frac{1}{8} [z]_{-2+|w|}^{2-|w|} = \frac{1}{8} \left( (2-|w|) - (-2+|w|) \right) \\
		&= \frac{1}{8} (4 - 2|w|) = \frac{2-|w|}{4}
	\end{align*}
	A densidade marginal de $W$ (√© uma distribui√ß√£o triangular) dada por:
	$$ f_W(w) = 
	\begin{cases}
		\frac{2-|w|}{4}, & \text{se } -2 < w < 2 \\
		0, &  \text{caso contr√°rio.}
	\end{cases}
	$$
	
	{\bf (d)} 
	A densidade condicional √© $f_{Z|W}(z|w) = \frac{f_{Z,W}(z,w)}{f_W(w)}$, para $f_W(w)>0$.
	$$
	f_{Z|W}(z|w) = \frac{1/8}{(2-|w|)/4} = \frac{4}{8(2-|w|)} = \frac{1}{2(2-|w|)}
	$$
	O suporte de $z$ dado $w$ √© $-2+|w| < z < 2-|w|$.
	$$
	f_{Z|W}(z|w) = 
	\begin{cases}
		\frac{1}{2(2-|w|)}, & \text{para } -2+|w| < z < 2-|w| \text{ e } w \in (-2,2) \\
		0, &  \text{caso contr√°rio.}
	\end{cases}
	$$
	Isto indica que, dado $W=w$, $Z$ segue uma distribui√ß√£o uniforme no intervalo $(-2+|w|, 2-|w|)$.
	


}



%\begin{problema}{B√¥nus}{pr5}
%Seja $Q = (x, y)$ um ponto escolhido aleatoriamente num disco unit√°rio centrado em $(0,0)$ e com raio 1. 
%
%\begin{itemize}
%	\item Qual o espa√ßo amostral do experimento?
%	\item Calcule a probabilidade de que $Q$ esteja a uma dist√¢ncia m√°xima de 0.5 do centro.
%	\item Calcule a probabilidade de $y > \frac{1}{\sqrt{2}}$;
%	\item Calcule a probabilidade de $|x - y| < 1$ e $|x + y| < 1$.
%\end{itemize}
%\end{problema}
%






\begin{problema}{}{pr-4}
	Considere  a {\it convolu√ß√£o} $f_X * f_Y$ entre as fun√ß√µes de densidade das vari√°veis aleat√≥rias  $X$ e $Y,$ i.e., a fun√ß√£o de densidade da vari√°vel aleat√≥ria $Z=X+Y.$ Mostre que o operador de convolu√ß√£o $ ( * )$ √©: 
	\begin{enumerate}
		\item comutativo: $f_X * f_Y = f_Y * f_X$
		\item distributivo: $f_Z*(f_X+f_Y ) = f_Z*f_X+f_Z*f_Y$
		\item  associativo: $(f_Z * f_X) * f_Y = f_Z *(f_X * f_Y )$
	\end{enumerate}
\end{problema}


{\small
	
A convolu√ß√£o de duas fun√ß√µes integr√°veis $g$ e $h$ √© definida como $(g * h)(t) = \int_{-\infty}^{\infty} g(x) h(t-x) \,dx$. Se $X$ e $Y$ s√£o v.a. independentes com fun√ß√µs de densidade $f_X$ e $f_Y$, a fun√ß√£o de densidade da soma $S=X+Y$ √© $f_S = f_X * f_Y$.
	
	{\bf (a):} Comutatividade. Seja
	$$ (f_X * f_Y)(t) = \int_{-\infty}^{\infty} f_X(x) f_Y(t-x) \,dx $$
	Fazendo a mudan√ßa de vari√°vel $u = t-x$. Assim, $x = t-u$ e $dx = -du$. Os limites de integra√ß√£o se invertem: quando $x \to \infty$, $u \to -\infty$ e quando $x \to -\infty$, $u \to \infty$.
	\begin{align*}
		(f_X * f_Y)(t) &= \int_{\infty}^{-\infty} f_X(t-u) f_Y(u) (-du) \\
		&= \int_{-\infty}^{\infty} f_Y(u) f_X(t-u) \,du \quad (\text{invertendo os limites e o sinal}) \\
		&= (f_Y * f_X)(t)
	\end{align*}
	Portanto, o operador de convolu√ß√£o √© comutativo.
	
	{\bf (b):} Distributividade
		\begin{align*}
		(f_Z * (f_X+f_Y))(t) &= \int_{-\infty}^{\infty} f_Z(x) (f_X+f_Y)(t-x) \,dx \\
		&= \int_{-\infty}^{\infty} f_Z(x) [f_X(t-x) + f_Y(t-x)] \,dx \quad (\text{soma de fun√ß√µes}) \\
		&= \int_{-\infty}^{\infty} [f_Z(x)f_X(t-x) + f_Z(x)f_Y(t-x)] \,dx \quad (\text{distributividade do produto}) \\
		&= \int_{-\infty}^{\infty} f_Z(x)f_X(t-x) \,dx + \int_{-\infty}^{\infty} f_Z(x)f_Y(t-x) \,dx \quad (\text{linearidade da integral}) \\
		&= (f_Z * f_X)(t) + (f_Z * f_Y)(t)
	\end{align*}
	Portanto, o operador √© distributivo sobre a adi√ß√£o.
	
	{\bf (c):} Associatividade
	Queremos provar que $((f_Z * f_X) * f_Y)(t) = (f_Z * (f_X * f_Y))(t)$.
	Come√ßamos pelo lado esquerdo. Seja $g = f_Z * f_X$, em que $g(y) = \int_{-\infty}^{\infty} f_Z(x) f_X(y-x) \,dx$.
	\begin{align*}
		((f_Z * f_X) * f_Y)(t) &= (g * f_Y)(t) = \int_{-\infty}^{\infty} g(y) f_Y(t-y) \,dy \\
		&= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} f_Z(x) f_X(y-x) \,dx \right) f_Y(t-y) \,dy
	\end{align*}
	Pelo Teorema de Fubini-Tonelli, podemos trocar a ordem de integra√ß√£o:
	$$ = \int_{-\infty}^{\infty} f_Z(x) \left( \int_{-\infty}^{\infty} f_X(y-x) f_Y(t-y) \,dy \right) \,dx $$
	Analisamos agora a integral interna. Fazemos a substitui√ß√£o $u = y-x$, o que implica $y = u+x$ e $dy = du$.
	$$ \int_{-\infty}^{\infty} f_X(u) f_Y(t-(u+x)) \,du = \int_{-\infty}^{\infty} f_X(u) f_Y((t-x)-u) \,du $$
	Esta integral √©, por defini√ß√£o, $(f_X * f_Y)(t-x)$. Substituindo de volta:
	$$ \int_{-\infty}^{\infty} f_Z(x) (f_X * f_Y)(t-x) \,dx $$
	Esta express√£o √© a defini√ß√£o de $(f_Z * (f_X * f_Y))(t)$. Portanto, a associatividade √© v√°lida.
}


%\bigskip
%\centering {\scshape\bf\textcolor{darkred}{BOA PROVA}}

%

\end{document}


%
%\begin{problema}{}{pr3}
%	Seja $X$ uma vari\'avel aleat\'oria com distribui\c{c}\~ao gama sob uma nova reparametriza\c{c}\~ao, aqui denotada por $X\sim G(\mu, \phi)$,
%	tal que
%	$$f(x;\mu, \phi) = {1\over\Gamma(\phi)}\left( {\phi x}\over{\mu}\right)^\phi\exp\left(-{{\phi x}\over{\mu}}\right){1\over x},\,\, x \geq 0, \mu >0, \phi> 0 \ \ \text{e} \ \ \Gamma(\phi) = \int^\infty_0 t^{(\phi - 1)}e^{-t} dt .$$  
%	\begin{enumerate}
%		\item Mostre que $f(x;\mu, \phi)$ forma uma fam\'{\i}lia exponencial bidimensional.
%		\item  Mostre que,  $$\mu = {\rm E}(X)\quad{\rm e}\quad \phi^{-1/2} = { \sqrt{{\rm Var}(X)}\over{\rm E}(X) }.$$
%		\item  Sejam  $X_1,\ldots,X_n$ uma amostra aleat\'oria de $X\sim G(\mu,\phi_1)$ 
%		e $Y_1,\ldots, Y_n$ uma amostra aleat\'oria de   $Y\sim G(\mu,\phi_2)$. Aqui,
%		$X$ e $Y$ s\~ao popula\c{c}\~oes independentes, 
%		$X \in \mathbb{R}^+$, $Y\in \mathbb{R}^+$, $\mu \in  \mathbb{R}^+$,  $\phi_1 \in \mathbb{R}^+$ e $\phi_2 \in \mathbb{R}^+.$
%		Determine a densidade conjunta $f_{(\xtill,\ytill)}{(\mu, \phi_1,\phi_2)}$.
%		\item  Mostre que a estat\'{\i}stica $$T =\Bigg(\sum_{i=1}^n X_i, \sum_{i=1}^n Y_i, \sum_{i=1}^n \log X_i, \sum_{i=1}^n\log Y_i \Bigg)$$ n\~ao \'e completa para a fam\'{\i}lia definida em 3.
%		\item O que acontece com as an√°lises anteriores se as distribui√ß√µes gama aqui consideradas s√£o curvas, i.e. quando $\mu=\phi_1=\phi_2$?
%	\end{enumerate}
%\end{problema}	
%
%\begin{problema}{}{pr-4}
%	Seja $\hat{\theta}$ um estimador n√£o viesado de um par√¢metro escalar $\theta \in \Theta_\theta \equiv \mathbb{R}$, satisfazendo $E_\theta(\hat{\theta}^2) < \infty$ para todo $\theta \in \Theta_\theta$. Dizemos que $\hat{\theta}$ √© um estimador de vari√¢ncia m√≠nima uniforme n√£o viesado (UMVU, do ingl√™s \textit{Uniform Minimum Variance Unbiased}) se 
%	\[
%	\text{Var}_\theta(\hat{\theta}) \leq \text{Var}_\theta(\tilde{\theta})
%	\]
%	para todo $\theta \in \Theta_\theta$ e qualquer outro estimador n√£o viesado $\tilde{\theta}$.  Considere o estimador $\hat{\theta}_\tau = \hat{\theta} + \tau U$, ou outro equivalente. Mostre que uma condi√ß√£o necess√°ria para que $\hat{\theta}$ seja um estimador UMVU √© que 
%	\[
%	E_\theta(\hat{\theta}U) = 0,
%	\]
%	para todo $\theta \in \Theta_\theta$ e para todos os estimadores $U$ com $E_\theta(U) = 0$ e $E_\theta(U^2) < \infty$. Assim, $\hat{\theta}$ deve ser "n√£o correlacionado com todo estimador n√£o viesado de zero".
%	
%	Agora, para $n \geq 2$, sejam $X_1, \dots, X_n$ vari√°veis independentes e identicamente distribu√≠das com distribui√ß√£o uniforme $U(\theta, 2\theta)$, para algum $\theta > 0$. 
%	
%	\begin{enumerate}
%		\item Mostre que $\tilde{\theta} = \frac{3}{2} X_1$ √© um estimador n√£o viesado de $\theta$.
%		\item Encontre um estimador n√£o viesado $\hat{\theta}$ que seja fun√ß√£o de uma estat√≠stica suficiente m√≠nima e que satisfa√ßa 
%		\[
%		\text{Var}_\theta(\hat{\theta}) < \text{Var}_\theta(\tilde{\theta}),
%		\]
%		para todo $\theta > 0$.
%		\item Verifique se $\hat{\theta}$ √© UMVU.
%	\end{enumerate}
%\end{problema}	
%
%\begin{problema}{}{pr-5}
%%Seja \( S \) uma estat√≠stica suficiente completa para um par√¢metro \( \theta \in \Theta_\theta \), e seja \( C \) uma estat√≠stica constante em distribui√ß√£o, i.e. uma estat√≠stica cuja distribui√ß√£o n√£o depende do par√¢metro desconhecido $\theta$ do modelo. Mostre que \( S \) e \( C \) s√£o independentes para cada \( \theta \in \Omega_\theta \). Qual √© o significado desse resultado para a infer√™ncia estat√≠stica sobre \( \theta \)?
%Seja  $X_1,\ldots
%,X_n$ uma amostra aleat\'oria de 
%$$f(x;\theta) = { 1 \over \theta_2}e^{-{(x -\theta_1)\over \theta_2 }}, \,\, \theta_1 \leq x <+\infty,\,\,\,
%-\infty < \theta_1 < +\infty, \,\, 0 < \theta_2 < +\infty.$$
%\begin{enumerate}
%	
%	\item Prove que esta \'e uma fam\'{\i}lia de loca\c{c}\~ao e escala.
%	\item Mostre que
%	$$V = {{X_{(n)} - X_{(1)}}\over{\sqrt{\sum_{i=1}^n(X_i -\overline X)^2}}}$$
%	\'e ancilar para a fam\'{\i}lia.
%	\item  Sejam 
%	$$Z = {X_{(n)} - X_{(1)}} \,\, {\rm e}\,\,\,T = {X_{(n)}}.$$ 
%	Prove que $Z$ e $T$ s\~ao vari\'aveis aleat\'orias independentes.
%	Aqui, $$X_{(n)} =\max\{X_1, \ldots,X_n \}   \quad \text{e} \quad X_{(1)} = \min\{X_1, \ldots,X_n \}$$ 
%\end{enumerate}
%\end{problema}	
%%\bibliography{references}
%

%\end{document}
%
%
%Abaixo est√° a solu√ß√£o detalhada para cada parte do problema:
%
%---
%
%### **Problema:**
%Bact√©rias s√£o distribu√≠das aleatoriamente em um fluido com densidade m√©dia \(\theta\) por unidade de volume (\(\theta \in H \subseteq [0, \infty)\)). A probabilidade de nenhuma bact√©ria estar presente em um volume \(v\) √©:
%\[
%P_\theta(\text{nenhuma bact√©ria no volume } v) = e^{-\theta v}.
%\]
%O objetivo √© decidir se h√° ou n√£o bact√©rias no fluido, com base em um teste aplicado a uma amostra de volume \(v\). Uma decis√£o incorreta resulta em perda de \(1\), enquanto uma decis√£o correta n√£o implica perda.
%
%---
%
%### **Parte (i): \(H = [0, \infty)\)**
%
%#### Regras de decis√£o n√£o randomizadas:
%Uma regra de decis√£o n√£o randomizada \(d\) decide entre:
%- \(d = 1\): Aceitar que h√° bact√©rias (\(\theta > 0\)).
%- \(d = 0\): Aceitar que n√£o h√° bact√©rias (\(\theta = 0\)).
%
%A decis√£o √© baseada no resultado do teste aplicado ao volume \(v\):
%- Se a amostra contiver bact√©rias, decidimos \(d = 1\).
%- Se a amostra n√£o contiver bact√©rias, decidimos \(d = 0\).
%
%#### Fun√ß√µes de risco:
%A fun√ß√£o de risco \(R(\theta, d)\) mede a perda esperada ao aplicar a regra \(d\):
%- Se \(d = 1\), a perda ocorre apenas se \(\theta = 0\), com probabilidade \(e^{-\theta v}\).
%- Se \(d = 0\), a perda ocorre se \(\theta > 0\), com probabilidade \(1 - e^{-\theta v}\).
%
%As fun√ß√µes de risco s√£o:
%\[
%R(\theta, d) =
%\begin{cases}
%	1 - e^{-\theta v}, & \text{se \(d = 0\)}, \\
%	e^{-\theta v}, & \text{se \(d = 1\)}.
%\end{cases}
%\]
%
%#### Regras admiss√≠veis:
%Uma regra de decis√£o \(d\) √© admiss√≠vel se n√£o existe outra regra com menor risco para todos os valores de \(\theta\). Ambas as regras \(d = 0\) e \(d = 1\) s√£o admiss√≠veis neste caso, pois minimizam o risco para intervalos complementares de \(\theta\).
%
%---
%
%### **Parte (ii): \(H = \{0, 1\}\)**
%
%#### Conjunto de risco \(S\):
%Se \(H = \{0, 1\}\), o conjunto de risco √©:
%\[
%S = \{(R(0, d), R(1, d)) : d \text{ √© uma regra randomizada}\}.
%\]
%
%Uma regra randomizada pode ser definida como \(d(X) = \alpha\), onde \(0 \leq \alpha \leq 1\) √© a probabilidade de decidir \(d = 1\).
%
%Os riscos s√£o:
%\[
%R(0, d) = \alpha,
%\]
%\[
%R(1, d) = 1 - \alpha e^{-v}.
%\]
%
%O conjunto de risco \(S\) cont√©m todos os pares \((\alpha, 1 - \alpha e^{-v})\) para \(0 \leq \alpha \leq 1\).
%
%#### Regra minimax:
%A regra minimax minimiza a m√°xima perda:
%\[
%\min_\alpha \max \{R(0, d), R(1, d)\}.
%\]
%
%Resolva:
%\[
%\alpha = \frac{1}{1 + e^{-v}}.
%\]
%
%Assim, a regra minimax √© \(d(X) = \frac{1}{1 + e^{-v}}\).
%
%---
%
%### **Parte (iii): \(H = [0, \infty)\), com prior \(\pi\):**
%
%#### Priori:
%\[
%\pi(\{0\}) = \frac{1}{3}, \quad \pi(A) = \frac{2}{3} \int_A e^{-\theta} \, d\theta, \quad A \subseteq (0, \infty).
%\]
%
%#### Regras de decis√£o de Bayes:
%A regra de Bayes minimiza o risco esperado sob a priori:
%\[
%R_\pi(d) = \frac{1}{3} R(0, d) + \frac{2}{3} \int_0^\infty R(\theta, d) e^{-\theta} \, d\theta.
%\]
%
%Para \(d = 1\):
%\[
%R_\pi(1) = \frac{1}{3} e^{-v}.
%\]
%
%Para \(d = 0\):
%\[
%R_\pi(0) = \frac{2}{3} \int_0^\infty (1 - e^{-\theta v}) e^{-\theta} \, d\theta.
%\]
%
%Calcule a integral:
%\[
%\int_0^\infty e^{-\theta v} e^{-\theta} \, d\theta = \int_0^\infty e^{-\theta (v+1)} \, d\theta = \frac{1}{v+1}.
%\]
%
%Substitua para obter:
%\[
%R_\pi(0) = \frac{2}{3} \left(1 - \frac{1}{v+1}\right) = \frac{2}{3} \frac{v}{v+1}.
%\]
%
%Compare os riscos e escolha a melhor regra.
%
%#### Risco de Bayes:
%Substitua a regra de Bayes na fun√ß√£o de risco para obter o risco correspondente.
%
%---
%
%### **Parte (iv): Custo \(v/24\):**
%
%O custo total √©:
%\[
%\text{Custo total} = R_\pi(d) + \frac{v}{24}.
%\]
%
%Otimize \(v\) resolvendo:
%\[
%\frac{\partial}{\partial v} \left(R_\pi(d) + \frac{v}{24}\right) = 0.
%\]
%
%Repita o c√°lculo para o custo por unidade de volume \(1/6\).
%
%---
%
%Se precisar de ajuda com os c√°lculos detalhados ou d√∫vidas espec√≠ficas, avise! 
%
%
%### **Parte I - Encontrar as Regras Admiss√≠veis**
%
%#### Problema:
%Estamos interessados em determinar todas as regras de decis√£o n√£o randomizadas e avaliar quais delas s√£o admiss√≠veis.
%
%1. **Regras N√£o Randomizadas:**
%As regras de decis√£o n√£o randomizadas para este problema s√£o baseadas nos resultados do teste realizado sobre o volume \(v\). Essas regras \(d\) podem ser definidas como:
%\[
%d = 1 \quad (\text{decidir que h√° bact√©rias/c√©lulas cancer√≠genas}),
%\]
%ou
%\[
%d = 0 \quad (\text{decidir que n√£o h√° bact√©rias/c√©lulas cancer√≠genas}).
%\]
%
%2. **Fun√ß√µes de Risco:**
%A fun√ß√£o de risco \(R(\theta, d)\) mede a perda esperada para cada decis√£o \(d\):
%- Para \(d = 0\) (decidir que n√£o h√° bact√©rias):
%\[
%R(\theta, 0) = \begin{cases}
%	0, & \text{se } \theta = 0, \\
%	1 - e^{-\theta v}, & \text{se } \theta > 0.
%\end{cases}
%\]
%- Para \(d = 1\) (decidir que h√° bact√©rias):
%\[
%R(\theta, 1) = \begin{cases}
%	1, & \text{se } \theta = 0, \\
%	e^{-\theta v}, & \text{se } \theta > 0.
%\end{cases}
%\]
%
%3. **Comparando os Riscos:**
%As regras admiss√≠veis s√£o aquelas que minimizam o risco \(R(\theta, d)\) para algum valor de \(\theta\) sem serem estritamente dominadas por outra regra.
%
%- Quando \(\theta = 0\), a decis√£o \(d = 0\) (nenhuma c√©lula presente) tem risco \(R(0, 0) = 0\), enquanto \(R(0, 1) = 1\). Assim, \(d = 0\) √© prefer√≠vel neste caso.
%- Quando \(\theta > 0\), a decis√£o \(d = 1\) (c√©lulas presentes) reduz o risco para \(R(\theta, 1) = e^{-\theta v}\), enquanto \(R(\theta, 0) = 1 - e^{-\theta v}\). Assim, \(d = 1\) √© prefer√≠vel para valores maiores de \(\theta\).
%
%4. **Conclus√£o:**
%- \(d = 0\) √© admiss√≠vel, pois minimiza o risco para \(\theta = 0\).
%- \(d = 1\) √© admiss√≠vel, pois minimiza o risco para \(\theta > 0\).
%- Nenhuma das regras √© estritamente dominante em todo o dom√≠nio de \(\theta\).
%
%---
%
%### **Parte IV - Determinar o Volume √ìtimo para Testar**
%
%#### Problema:
%Queremos encontrar o volume \(v\) que minimiza o custo total, que inclui o risco esperado e o custo do teste. 
%
%1. **Custo Total:**
%O custo total √© dado por:
%\[
%\text{Custo Total}(v) = R_\pi(d, v) + \frac{v}{24},
%\]
%onde \(R_\pi(d, v)\) √© o risco esperado sob a prior \(\pi\).
%
%#### Calculando o Risco Esperado \(R_\pi(d, v)\):
%De acordo com a prior fornecida:
%\[
%\pi(\{0\}) = \frac{1}{3}, \quad \pi(A) = \frac{2}{3} \int_A e^{-\theta} \, d\theta.
%\]
%
%Para a regra \(d = 1\):
%\[
%R_\pi(1, v) = \frac{1}{3} R(0, 1) + \frac{2}{3} \int_0^\infty R(\theta, 1) e^{-\theta} \, d\theta.
%\]
%Substitu√≠mos:
%\[
%R(0, 1) = 1, \quad R(\theta, 1) = e^{-\theta v}.
%\]
%
%\[
%R_\pi(1, v) = \frac{1}{3} \cdot 1 + \frac{2}{3} \int_0^\infty e^{-\theta v} e^{-\theta} \, d\theta.
%\]
%
%A integral √©:
%\[
%\int_0^\infty e^{-\theta (v+1)} \, d\theta = \frac{1}{v+1}.
%\]
%
%Assim:
%\[
%R_\pi(1, v) = \frac{1}{3} + \frac{2}{3} \cdot \frac{1}{v+1}.
%\]
%
%Para a regra \(d = 0\), calcule:
%\[
%R_\pi(0, v) = \frac{1}{3} R(0, 0) + \frac{2}{3} \int_0^\infty R(\theta, 0) e^{-\theta} \, d\theta.
%\]
%
%Similarmente, resolvemos e determinamos \(R_\pi(0, v)\).
%
%#### Minimiza√ß√£o:
%Agora, o custo total √©:
%\[
%\text{Custo Total}(v) = \frac{1}{3} + \frac{2}{3} \cdot \frac{1}{v+1} + \frac{v}{24}.
%\]
%
%Derivamos em rela√ß√£o a \(v\):
%\[
%\frac{\partial}{\partial v} \left(\frac{1}{3} + \frac{2}{3} \cdot \frac{1}{v+1} + \frac{v}{24}\right) = 0.
%\]
%
%A derivada √©:
%\[
%-\frac{2}{3} \cdot \frac{1}{(v+1)^2} + \frac{1}{24} = 0.
%\]
%
%Multiplicamos por \(24(v+1)^2\) para simplificar:
%\[
%-16 + (v+1)^2 = 0.
%\]
%
%Resolvemos:
%\[
%(v+1)^2 = 16 \quad \Rightarrow \quad v+1 = 4 \quad \Rightarrow \quad v = 3.
%\]
%
%#### Resposta para Custo \(v/24\):
%O volume √≥timo para testar √© \(v = 3\).
%
%#### Para o Custo \(1/6\) por Unidade de Volume:
%Se o custo for \(v/6\), o termo \(\frac{v}{24}\) muda para \(\frac{v}{6}\). Repita o processo para obter o novo volume √≥timo.
%
%---
%
%Se precisar de mais explica√ß√µes ou detalhes, estarei √† disposi√ß√£o! üòä